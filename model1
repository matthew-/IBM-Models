#!/usr/bin/env python
# Usage: $model1 -d path/to/data > alignments.out
# Out should look rather incomprehensible, as it's meant to be passed to the grade file
# For each sentence you'll get an alignment (foreign-english)  like so
"""
0-0 1-22 2-0 3-27 4-0 5-17 6-0 7-0 8-8 9-8 10-0 11-0 12-8 13-0 14-8 15-17 16-0 17-0 18-27 19-0 20-8 21-17 22-0 23-8 24-0 25-0 26-17 27-0 28-27
"""
############################################################################
############  TODO: Add option for null token to foreign sentence ##########
############        Optimize (still painfully slow n > 1000       ##########
############################################################################
import optparse
import sys
from collections import defaultdict
from itertools import izip, count
from pprint import pprint

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English (Target) filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French (Source/Foreign) filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-I", "--Iterations", dest="num_iterations", default=5, type="int", help="Number of iterations to run EM algorithm.")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)
num_iterations = opts.num_iterations

sys.stderr.write("Training with IBM Model 1...\n")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]] # Read in data
f_count = defaultdict(float)
e_count = defaultdict(int)
fe_count = defaultdict(int)
fe_prob = defaultdict(float)
s_total = defaultdict(float)
t = defaultdict(float)

# Itialize alignment probabilities uniformly
all_e = []  
for (f,e) in bitext:
  for e_w in set(e):
    all_e += [e_w]
for (f,e) in bitext:
  for e_w in e:
    for f_w in f:
      t[(e_w,f_w)] = 1.0/len(set(all_e))
    
for N in range(1,num_iterations): 
  sys.stderr.write("Beginning iteration: %i\n" % N)
  # zero out counts
  for it in fe_count.values():
    it = 0
  for it in f_count.values():
    it = 0
  for (n, (f,e)) in izip(count(),bitext):
    # compute normalization ?!
    for e_i in set(e):
      s_total[e_i] = 0 
      for f_i in set(f):
        s_total[e_i] += t[(e_i,f_i)]
    for e_i in set(e):
      for f_i in set(f):
        fe_count[(e_i,f_i)] += t[(e_i,f_i)] / s_total[e_i]
        f_count[f_i] += t[(e_i,f_i)] / s_total[e_i]

  for f_i in sorted(f_count.keys()):
    for e_i in sorted(s_total.keys()):
      t[(e_i,f_i)] = fe_count[(e_i,f_i)] / f_count[f_i]
  N += 1

for (f,e) in bitext:
  ii =0
  for(i,f_i) in enumerate(f):
    jj = 0
    p_max = 0
    for(j,e_j) in enumerate(e):
      if  t[(e_j,f_i)] > p_max:
        p_max = t[(e_j,f_i)]
        ii = i
        jj = j 
    print  "%i-%i" %(ii,jj),
  print
      
